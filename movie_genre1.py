# -*- coding: utf-8 -*-
"""movie_genre1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cMp8G7qQs6HoJpSJk3_IbtH-v7ljPW4H

Data Collection - *Web Scraping IMSDB*

General Genres
* Action
* Comedy
* Drama
* Fantasy
* Horror
* Romance

Imports
"""

import requests
import random
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import brown
nltk.download('brown')
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
nltk.download('stopwords')
import nltk
nltk.download('punkt')
from nltk.probability import FreqDist
import re
import pandas as pd
from nltk.tokenize import sent_tokenize, word_tokenize
import csv
from google.colab import drive
import string
import numpy as np

from google.colab import auth
from google.colab import drive
drive.mount('/content/drive')
import gspread
from google.auth import default
# authenticate
auth.authenticate_user()
from oauth2client.client import GoogleCredentials as GC
# gc = gspread.authorize(GC.get_application_default())
creds, _ = default()
gc = gspread.authorize(creds)
# create, and save df
from gspread_dataframe import set_with_dataframe

"""Extracting unique Shrek words [test case]"""

shrek = 'https://imsdb.com/scripts/Shrek.html'

# check whether we have access or not
page = requests.get(shrek)

# yes we do!
# print(page.status_code)
# print(page.content)

from bs4 import BeautifulSoup

soup = BeautifulSoup(page.content, "html.parser")
container = soup.find(id="mainbody")
# print(container.prettify())

script_elements = soup.find_all('pre')
# speakers = soup.find_all('b')

for para in soup.find_all("pre"):
  extracted = para.get_text()

# Text Mining
import nltk
nltk.download('punkt')

from nltk.tokenize import sent_tokenize
sent_tokenize(extracted)

## Word Tokenization
from nltk.tokenize import word_tokenize

shrek_words = word_tokenize(extracted)

# Frequency Distribution
from nltk.probability import FreqDist
# pre = len(shrek_words)
FreqDist(shrek_words)
shrek_words_1 = []
for i in range(len(shrek_words)):
    if shrek_words[i].isupper() == True:
      continue
    else:
      shrek_words[i] = shrek_words[i].lower()
      shrek_words_1.append(shrek_words[i]) # make new list of non-speaker shrek words
# post = len(shrek_words_1)
# print(pre, post) # got rid of 16781-15538 = 1243 words = speaker names + unnecessary which were all uppercase words

from nltk.corpus import stopwords
nltk.download('stopwords')

# Remove Stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)
# len(shrek_words) #16781

## Removing punctuation

shrek_words_1 = [word for word in shrek_words_1 if word.isalpha()]
# len(words) #12860

## Removing stopwords

shrek_words_1 = [word for word in shrek_words_1 if word not in stop_words]
# len(words2) #11974

# len(shrek_words_1) # 5985

# Lemmatization
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')

lem = WordNetLemmatizer()

lemmatized_shrek_words = [lem.lemmatize(word) for word in shrek_words_1]
lemmatized_shrek_words
FreqDist(lemmatized_shrek_words).most_common(20)
# len(lemmatized_shrek_words) #5985

"""Function LINKS TO ALL READ SCRIPT LINKS"""

def get_links(genre):
  random.seed(20)
  action_link = 'https://imsdb.com/genre/' + genre
  page = requests.get(action_link)
  soup = BeautifulSoup(page.content, 'html.parser')
  container = soup.find(id = 'mainbody')
  list_action = soup.find_all('a', href = True)
  list_actions =[]
  for para in soup.find_all("a",href = True):
    list_actions.append(para['href'])
    new_list_actions = list_actions[63:]
    list_actions_test=[]
    #test 336 total
  #for i in range(20):
  list_actions_test = random.sample(new_list_actions,15)
    #create link to each page
  links_list_actions_test = []

  for i in range(len(list_actions_test)):
    links_list_actions_test.append('https://imsdb.com/Movie%20Scripts'+ list_actions_test[i])
    #Now find the scripts links
    #print(links_list_actions_test[0])
    script_links = []

  script = []
  for i in range(len(links_list_actions_test)):
    page2 = requests.get(links_list_actions_test[i])
    soup2 = BeautifulSoup(page2.content, 'html.parser')
    container = soup2.find(id = 'mainbody')
    for para2 in soup2.find_all("a", href= True):
      if ('Read' in para2.get_text()):
        script.append(para2['href'])

  for i in range(len(script)):
    script_links.append('https://imsdb.com'+ script[i])
  return(script_links)

"""Genre LINKS TO READ SCRIPTS"""

random.seed(20)
genere_links =['Action', 'Comedy', 'Drama', 'Fantasy','Horror', 'Mystery','Romance','Thriller','Western']
action_scripts = get_links(genere_links[0])
comedy_scripts = get_links(genere_links[1])
drama_scripts = get_links(genere_links[2])
fanatsy_scripts = get_links(genere_links[3])
horror_scripts = get_links(genere_links[4])
mystery_scripts = get_links(genere_links[5])
romance_scripts = get_links(genere_links[6])
thriller_scripts = get_links(genere_links[7])
western_scripts = get_links(genere_links[8])

"""

```
# This is formatted as code
```

# Extract Scripts of Each Genre"""

#new function for copile scripts
stop_words = set(stopwords.words("english"))
lem = WordNetLemmatizer()
i = 0
names_text = set(line.strip().lower() for line in open('/content/names.txt'))
last_names = set(line.strip().lower() for line in open('/content/last-names.txt'))

#speaker_names = set()
def extract_script(script_type):
  speaker_names = set()
  # script_words = []
  script = []
  for i in range(len(script_type)):
    page = requests.get(script_type[i])
    soup = BeautifulSoup(page.content, "html.parser")
    container = soup.find(id="mainbody")
    for para in soup.find_all("pre"):
      #para.get_text()
      extracted = para.get_text()

    extracted = re.sub(r'\r\n', '', extracted)
    n_snames =[]
    snames = re.findall(r'[A-Z]+[A-Z]+[A-Z]*[\s]+', extracted)


    words = word_tokenize(extracted)
    alphabet = set(string.ascii_lowercase)
    #result = [line.rstrip() for line in lines]
    words_1 = []
    ## Remove headers (all uppercase words)
    for i in range(len(words)):
      if words[i].isupper() == True:
       #speaker_names.update(words[i].lower())
       continue
      else:
        words[i] = words[i].lower()
        words_1.append(words[i])
    ## Removing punctuation
    words_1 = [word for word in words_1 if word.isalpha()]
    #speaker_names = [word for word in speaker_names if word.isalpha()]
    speaker_names = set(n_snames)
    ## Removing stopwords
    words_1 = [word for word in words_1 if word not in stop_words]
    words_1 = [word for word in words_1 if word not in names_text]
    words_1 = [word for word in words_1 if word not in last_names]
    words_1 = [word for word in words_1 if word not in speaker_names]
    words_1 = [word for word in words_1 if word not in alphabet]
    lemmatized_words = [lem.lemmatize(word) for word in words_1]
    script_words = lemmatized_words
    freq_dist = FreqDist(script_words)
    # script.append(freq_dist)
    script.append(script_words)
  return script

action = extract_script(action_scripts)
comedy = extract_script(comedy_scripts)
drama = extract_script(drama_scripts)
fantasy = extract_script(fanatsy_scripts)
horror = extract_script(horror_scripts)
romance = extract_script(romance_scripts)

# Example
print(action_scripts)
# then do each on in a csv file or tab seprated of all scripts with what Caryn did below

# action.to_csv('action.csv')

"""Attaching unique words for each genre to gsheet [examine words]"""

## Attaching unique words for each genre to gsheet [examine words]

# m = []
# for i in range(len(action_scripts)):
#   m = m + list(FreqDist(action[i]).most_common(10))
# action_df = pd.DataFrame(data = m).groupby(0)[1].apply(sum).to_frame().reset_index()
# r = 'movie_genre'
# # gc.create(r)  # if not exist
# sheet = gc.open(r).sheet1
# set_with_dataframe(sheet, action_df)

# m = []
# for i in range(len(romance_scripts)):
#   m = m + list(FreqDist(romance[i]).most_common(10))
# romance_df = pd.DataFrame(data = m).groupby(0)[1].apply(sum).to_frame().reset_index()
# r = 'movie_genre'
# # gc.create(r)  # if not exist
# sheet = gc.open(r).sheet1
# set_with_dataframe(sheet, romance_df)

# m = []
# for i in range(len(fanatsy_scripts)):
#   m = m + list(FreqDist(fantasy[i]).most_common(10))
# fantasy_df = pd.DataFrame(data = m).groupby(0)[1].apply(sum).to_frame().reset_index()
# r = 'movie_genre'
# # gc.create(r)  # if not exist
# sheet = gc.open(r).sheet1
# set_with_dataframe(sheet, fantasy_df)

# m = []
# for i in range(len(horror_scripts)):
#   m = m + list(FreqDist(horror[i]).most_common(10))
# horror_df = pd.DataFrame(data = m).groupby(0)[1].apply(sum).to_frame().reset_index()
# r = 'movie_genre'
# # gc.create(r)  # if not exist
# sheet = gc.open(r).sheet1
# set_with_dataframe(sheet, horror_df)

# m = []
# for i in range(len(comedy_scripts)):
#   m = m + list(FreqDist(comedy[i]).most_common(10))
# comedy_df = pd.DataFrame(data = m).groupby(0)[1].apply(sum).to_frame().reset_index()
# r = 'movie_genre'
# # gc.create(r)  # if not exist
# sheet = gc.open(r).sheet1
# set_with_dataframe(sheet, comedy_df)

# m = []
# for i in range(len(drama_scripts)):
#   m = m + list(FreqDist(drama[i]).most_common(10))
# drama_df = pd.DataFrame(data = m).groupby(0)[1].apply(sum).to_frame().reset_index()
# r = 'movie_genre'
# # gc.create(r)  # if not exist
# sheet = gc.open(r).sheet1
# set_with_dataframe(sheet, drama_df)

"""Bar Graphs of the Most common words for each Genre"""

# finds the 20 most common words + frequencies of each of the action scripts and adds them to a dataframe
m = []
i = 0
for i in range(len(action_scripts)):
  m = m + list(FreqDist(action[i]).most_common(2))
action_df = pd.DataFrame(columns = ['words','action_count'], data = m).groupby('words')['action_count'].apply(sum).to_frame()
action_df.plot(y = 'action_count', kind = 'bar', title = 'Action Genre: Most Common Words', color = 'green')


m =[]
for i in range(len(fanatsy_scripts)):
  m = m + list(FreqDist(fantasy[i]).most_common(2))
fantasy_df = pd.DataFrame(columns = ['words', 'fantasy_count'], data = m).groupby('words')['fantasy_count'].apply(sum).to_frame()
fantasy_df.plot(y = 'fantasy_count', kind = 'bar', title = 'Fantasy Genre: Most Common Words', color = 'yellow')

m =[]
for i in range(len(horror_scripts)):
  m = m + list(FreqDist(horror[i]).most_common(2))
horror_df = pd.DataFrame(columns = ['words', 'horror_count'], data = m).groupby('words')['horror_count'].apply(sum).to_frame()
horror_df.plot(y = 'horror_count', kind = 'bar', title = 'Horror Genre: Most Common Words', color = 'black')

m =[]
for i in range(len(comedy_scripts)):
  m = m + list(FreqDist(comedy[i]).most_common(2))
comedy_df = pd.DataFrame(columns = ['words', 'comedy_count'], data = m).groupby('words')['comedy_count'].apply(sum).to_frame()
comedy_df.plot(y = 'comedy_count', kind = 'bar', title = 'Comedy Genre: Most Common Words', color = 'orange')

m =[]
for i in range(len(drama_scripts)):
  m = m + list(FreqDist(drama[i]).most_common(2))
drama_df = pd.DataFrame(columns = ['words', 'drama_count'], data = m).groupby('words')['drama_count'].apply(sum).to_frame()
drama_df.plot(y = 'drama_count', kind = 'bar', title = 'Drama Genre: Most Common Words', color = 'purple')

m =[]
for i in range(len(romance_scripts)):
  m = m + list(FreqDist(romance[i]).most_common(2))
romance_df = pd.DataFrame(columns = ['words', 'romance_count'], data = m).groupby('words')['romance_count'].apply(sum).to_frame()
romance_df.plot(y = 'romance_count', kind = 'bar', title = 'Romance Genre: Most Common Words', color = 'pink')

drama_df
horror_df

"""Bag-Of-Words Matrix"""

# Commented out IPython magic to ensure Python compatibility.
# bag-of-words
dfs = [action_df, comedy_df, horror_df, romance_df, drama_df, fantasy_df]
tot_dfs = pd.concat(dfs,axis=1).fillna(0)
copy_dfs = tot_dfs.copy()
tot_dfs

import numpy as np
from pandas import DataFrame
import seaborn as sns
# %matplotlib inline
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 20))
tot_dfs.columns = ['action','comedy', 'horror', 'romance', 'drama', 'fantasy']
heat = pd.DataFrame(data = tot_dfs, index = tot_dfs.index, columns = tot_dfs.columns)
sns.heatmap(heat, annot=False, linewidths =0)

"""One Hot Encoding Matrix"""

# one hot encoding
one_hot_dfs = tot_dfs.copy()
one_hot_dfs[one_hot_dfs>0] = 1
one_hot_dfs['sum'] = 0
one_hot_dfs['sum'] = one_hot_dfs.sum(axis=1)
one_hot_dfs
# one_hot_dfs[one_hot_dfs['sum']==6]

#sum each row using bag of words for term frequency
TF_copy = tot_dfs.transpose().copy()
# initializing sum prevents sum to add on to itself (which is why we kept getting multiples of two whenever we bug fix)
TF_copy['sum'] = 0
TF_copy['sum'] = TF_copy.sum(axis=1)

TF_copy

# term frequency using raw count / total words
TF_copy = TF_copy.iloc[:,:].div(TF_copy['sum'], axis = 0).drop('sum', axis=1)

TF_copy.index = ['TF_action', 'TF_comedy', 'TF_horror', 'TF_romance', 'TF_drama', 'TF_fantasy']

TF_copy

"""Function of extracting words"""

# adding IDF column
IDF_copy = TF_copy.copy().transpose()

#IDF = ln(total number of genres (6) / number of genres containing words)
#use one-hot-encoding
IDF_copy['IDF'] = 0
IDF_copy['IDF'] = np.log(6/one_hot_dfs['sum'])
IDF_copy

# vectorizing TFIDF values
TFIDF = IDF_copy.copy()
TFIDF

# multiply TF * IDF
TFIDF = TFIDF.iloc[:,:].multiply(TFIDF['IDF'], axis = 0).drop('IDF', axis=1)
TFIDF.columns = ['TFIDF_action', 'TFIDF_comedy', 'TFIDF_horror', 'TFIDF_romance', 'TFIDF_drama', 'TFIDF_fantasy']
# TFIDF training model
TFIDF

TFIDF_df = TFIDF.copy()
# rename TFIDF_genre to mappable values
# action=1, comedy=2, horror=3, romance=4, drama=5, fantasy=6
TFIDF_df.columns = [1, 2, 3, 4, 5, 6]
# convert dataframe to two column structure with the index values being the genre
TFIDF_df = TFIDF_df.transpose().stack().reset_index()
TFIDF_df.columns = ['genre','words','TFIDF']
TFIDF_graphs = TFIDF_df.copy()
codes, uniques = pd.factorize(TFIDF_df.words)
TFIDF_df['words']=codes+1
TFIDF_df.set_index('genre')

# graphs of TF-IDF per genre
TFIDF_graphs[(TFIDF_graphs['genre']==1)&(TFIDF_graphs['TFIDF']>0)].plot(kind = 'bar', x = 'words', y = 'TFIDF', color = 'green', title = 'Action Genre: TF-IDF Words')
TFIDF_graphs[(TFIDF_graphs['genre']==2)&(TFIDF_graphs['TFIDF']>0)].plot(kind = 'bar', x = 'words', y = 'TFIDF', color = 'orange', title = 'Comedy Genre: TF-IDF Words')
TFIDF_graphs[(TFIDF_graphs['genre']==3)&(TFIDF_graphs['TFIDF']>0)].plot(kind = 'bar', x = 'words', y = 'TFIDF', color = 'black', title = 'Horror Genre: TF-IDF Words')
TFIDF_graphs[(TFIDF_graphs['genre']==4)&(TFIDF_graphs['TFIDF']>0)].plot(kind = 'bar', x = 'words', y = 'TFIDF', color = 'pink', title = 'Romance Genre: TF-IDF Words')
TFIDF_graphs[(TFIDF_graphs['genre']==5)&(TFIDF_graphs['TFIDF']>0)].plot(kind = 'bar', x = 'words', y = 'TFIDF', color = 'purple', title = 'Drama Genre: TF-IDF Words')
TFIDF_graphs[(TFIDF_graphs['genre']==6)&(TFIDF_graphs['TFIDF']>0)].plot(kind = 'bar', x = 'words', y = 'TFIDF', color = 'yellow', title = 'Fantasy Genre: TF-IDF Words')

"""Test Data"""

genere_links =['Action', 'Comedy', 'Drama', 'Fantasy','Horror','Romance']
random.seed(105)

# By default, a module has some hidden variables defined
#print({k: v for k, v in globals().items() if not k.startswith("__")})
for i in range(len(genere_links)):
    globals()[f"{genere_links[i].lower()}_test_scripts"] = get_links(genere_links[i])
#for test
action_test = extract_script(action_test_scripts)
comedy_test = extract_script(comedy_test_scripts)
drama_test = extract_script(drama_test_scripts)
fantasy_test = extract_script(fantasy_test_scripts)
horror_test = extract_script(horror_test_scripts)
romance_test = extract_script(romance_test_scripts)
# finds the 20 most common words + frequencies of each of the action scripts and adds them to a dataframe
m = []
i = 0
for i in range(len(action_test)):
  m = m + list(FreqDist(action_test[i]).most_common(2))
action_test_df = pd.DataFrame(columns = ['words','action_count'], data = m).groupby('words')['action_count'].apply(sum).to_frame()
action_test_df.plot(y = 'action_count', kind = 'bar', title = 'Action Genre: Most Common Words', color = 'green')


m =[]
for i in range(len(fantasy_test)):
  m = m + list(FreqDist(fantasy_test[i]).most_common(2))
fantasy_test_df = pd.DataFrame(columns = ['words', 'fantasy_count'], data = m).groupby('words')['fantasy_count'].apply(sum).to_frame()
fantasy_test_df.plot(y = 'fantasy_count', kind = 'bar', title = 'Fantasy Genre: Most Common Words', color = 'yellow')

m =[]
for i in range(len(horror_scripts)):
  m = m + list(FreqDist(horror_test[i]).most_common(2))
horror_test_df = pd.DataFrame(columns = ['words', 'horror_count'], data = m).groupby('words')['horror_count'].apply(sum).to_frame()
horror_test_df.plot(y = 'horror_count', kind = 'bar', title = 'Horror Genre: Most Common Words', color = 'black')

m =[]
for i in range(len(comedy_test)):
  m = m + list(FreqDist(comedy_test[i]).most_common(2))
comedy_test_df = pd.DataFrame(columns = ['words', 'comedy_count'], data = m).groupby('words')['comedy_count'].apply(sum).to_frame()
comedy_test_df.plot(y = 'comedy_count', kind = 'bar', title = 'Comedy Genre: Most Common Words', color = 'orange')

m =[]
for i in range(len(drama_test)):
  m = m + list(FreqDist(drama_test[i]).most_common(2))
drama_test_df = pd.DataFrame(columns = ['words', 'drama_count'], data = m).groupby('words')['drama_count'].apply(sum).to_frame()
drama_test_df.plot(y = 'drama_count', kind = 'bar', title = 'Drama Genre: Most Common Words', color = 'purple')

m =[]
for i in range(len(romance_test)):
  m = m + list(FreqDist(romance_test[i]).most_common(2))
romance_test_df = pd.DataFrame(columns = ['words', 'romance_count'], data = m).groupby('words')['romance_count'].apply(sum).to_frame()
romance_test_df.plot(y = 'romance_count', kind = 'bar', title = 'Romance Genre: Most Common Words', color = 'pink')

# bag-of-words
dfs = [action_test_df, comedy_test_df, horror_test_df, romance_test_df, drama_test_df, fantasy_test_df]
tot_test_dfs = pd.concat(dfs,axis=1).fillna(0)
copy_test_dfs = tot_test_dfs.copy()
tot_test_dfs
# one hot encoding
test_one_hot_dfs = tot_test_dfs.copy()
test_one_hot_dfs[test_one_hot_dfs>0] = 1
test_one_hot_dfs['sum'] = 0
test_one_hot_dfs['sum'] = test_one_hot_dfs.sum(axis=1)
test_one_hot_dfs
#sum each row using bag of words for term frequency
test_TF_copy = tot_test_dfs.transpose().copy()
# initializing sum prevents sum to add on to itself (which is why we kept getting multiples of two whenever we bug fix)
test_TF_copy['sum'] = 0
test_TF_copy['sum'] = test_TF_copy.sum(axis=1)
# term frequency using raw count / total words
test_TF_copy = test_TF_copy.iloc[:,:].div(test_TF_copy['sum'], axis = 0).drop('sum', axis=1)
test_TF_copy.index = ['TF_action', 'TF_comedy', 'TF_horror', 'TF_romance', 'TF_drama', 'TF_fantasy']
# adding IDF column
test_IDF_copy = test_TF_copy.copy().transpose()

#IDF = ln(total number of genres (6) / number of genres containing words)
#use one-hot-encoding
test_IDF_copy['IDF'] = 0
test_IDF_copy['IDF'] = np.log(6/test_one_hot_dfs['sum'])
test_IDF_copy
# vectorizing TFIDF values
test_TFIDF = test_IDF_copy.copy()
test_TFIDF
# multiply TF * IDF
test_TFIDF = test_TFIDF.iloc[:,:].multiply(test_TFIDF['IDF'], axis = 0).drop('IDF', axis=1)
test_TFIDF.columns = ['TFIDF_action', 'TFIDF_comedy', 'TFIDF_horror', 'TFIDF_romance', 'TFIDF_drama', 'TFIDF_fantasy']
# TFIDF training model
test_TFIDF
test_TFIDF_df = test_TFIDF.copy()
# rename TFIDF_genre to mappable values
# action=1, comedy=2, horror=3, romance=4, drama=5, fantasy=6
test_TFIDF_df.columns = [1, 2, 3, 4, 5, 6]
# convert dataframe to two column structure with the index values being the genre
test_TFIDF_df = test_TFIDF_df.transpose().stack().reset_index()
test_TFIDF_df.columns = ['genre','words','TFIDF']
#converts all unique words to mappable values keys into codes
# codes stores the keys
# uniques stores the unique words
codes, uniques = pd.factorize(test_TFIDF_df.words)
#adds a value of 1 to all key values
test_TFIDF_df['words']=codes+1
#set index to word key
test_TFIDF_df.set_index('words')

# train 80%, test 20%
TFIDF_train = TFIDF_df.copy()
TFIDF_test  = test_TFIDF_df.copy()

TFIDF_train.plot.scatter(x='TFIDF', y='genre')
TFIDF_test.plot.scatter(x='TFIDF', y='genre')

"""Test"""

# linear regression model?
from sklearn.linear_model import LinearRegression
X_train = TFIDF_train[['TFIDF', 'words']]
X_test = TFIDF_test[['TFIDF', 'words']]
y_train = TFIDF_train[['genre']]

model = LinearRegression(fit_intercept=False)
model.fit(X=X_train, y=y_train)
model.predict(X=X_test)



# from numpy.core.fromnumeric import transpose
# #TF-IDF = (1 + log term frequency) * log(1 + # of documents / number of occurences of term in all documents)
# import math
# #def computeTF(temp_df, counts):
# def computeTF(temp_df):
#   tfDict = []
#   temp_words = []
#   for i in range(len(temp_df)):
#     value = temp_df.action_count[i]
#     tfDict.append(value/float(length))
#     temp_words.append(temp_df.index[i])
#   my_array = np.array([temp_words,tfDict])
#   df = pd.DataFrame(my_array, columns = len(temp_df))
#   return df

# def computeIDF():
#   one_hot_dfs['idf'] = np.log(6/one_hot_dfs['sum'])

# IDF_test = computeIDF()
# one_hot_dfs

# tot_dfs.sum(axis=0)

# copy_dfs = copy_dfs.transpose()

# copy_dfs['sum'] = copy_dfs.sum(axis=1)
# tf_copy_dfs = copy_dfs.copy() #copying the copy_dfs
# copy_dfs
# new_copy_df = copy_dfs.iloc[:,:].div(copy_dfs['sum'], axis = 0)
# new_copy_df

# new_copy_df2 = new_copy_df.transpose()

# TF_IDF_df = new_copy_df2.iloc[:,:].multiply(one_hot_dfs['idf'], axis=0)
# TF_IDF_df.drop('sum')

#classification table

"""## Test"""

keys = pd.DataFrame(pd.factorize(uniques)).transpose()
keys.columns = ['key', 'words']
keys['key'] += 1
keys.set_index('key')

script = ['https://imsdb.com/scripts/Avengers,-The-(2012).html']
avengers_df = pd.DataFrame(columns = ['words', 'avenger_counts'],data = list(FreqDist(extract_script(script)[0]).most_common(10))).set_index('words')
#TF of avengers
avengers_df = avengers_df.iloc[:,:].div(avengers_df['avenger_counts'].sum())
avengers_df.columns = ['avenger_TF']

avenger_key = []
for p in range(len(avengers_df['avenger_TF'])):
  found = False
  for i in range(len(keys)):
    if keys['words'][i] == avengers_df.index[p]:
      avenger_key.append(keys['key'][i])
      found = True
  if found == False:
    avenger_key.append(0)
avengers_df['key'] = avenger_key

avengers_df['predict'] = model.predict(avengers_df)
avengers_df['predict'] = round(avengers_df['predict'])
# avengers_df.loc[len(avengers_df.index)] = [avengers_df['avenger_TF'].mean(), avengers_df['key'].mean(),avengers_df['predict'].mode()]
avengers_df[avengers_df['key']!=0]

# Testing Crazy, Stupid Love (Comedy, Drama, Romance)
script = ['https://imsdb.com/scripts/Crazy,-Stupid,-Love.html']
csl_df = pd.DataFrame(columns = ['words', 'csl_counts'],data = list(FreqDist(extract_script(script)[0]).most_common(10))).set_index('words')
#TF of csl
csl_df = csl_df.iloc[:,:].div(csl_df['csl_counts'].sum())
csl_df.columns = ['csl_TF']

csl_key = []
for p in range(len(csl_df['csl_TF'])):
  found = False
  for i in range(len(keys)):
    if keys['words'][i] == csl_df.index[p]:
      csl_key.append(keys['key'][i])
      found = True
  if found == False:
    csl_key.append(0)
csl_df['key'] = csl_key

csl_df['predict'] = model.predict(csl_df)
csl_df['predict'] = round(csl_df['predict'])
csl_df[csl_df['key']!=0]

#around
model.predict(
    pd.DataFrame({'TFIDF':0.083019,'words':1}, index={0})
    )

#one
model.predict(
    pd.DataFrame({'TFIDF':0.071698,'words':10}, index={0})
    )

#run
model.predict(
    pd.DataFrame({'TFIDF':0.066038,'words':11}, index={0})
    )

#got
model.predict(
    pd.DataFrame({'TFIDF':0.105660,'words':36}, index={0})
    )
# action=1, comedy=2, horror=3, romance=4, drama=5, fantasy=6

"""three actions, one romance => ACTION????"""

# # new_copy_df2.iloc[:,:].multiply(one_hot_dfs['idf'], axis=0)
# avengers_df['tf'] = avengers_df.iloc[:,:].div(avengers_df.sum(), axis=1)
# avengers_df['idf'] = np.log(6/one_hot_dfs['sum'])
# avengers_df['tfidf'] = avengers_df.iloc[:,2].multiply(avengers_df['idf'])
# avengers_df

"""PDF conversion"""

# turning this to pdf
!pip install pypandoc
!pip install kaleido
!pip install plotly>=4.0.0.
!wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64. AppImage -O /usr/local/bin/orca.
!chmod +x /usr/local/bin/orca.
!apt-get install xvfb libgtk2.0-0 libgconf-2-4.
!sudo apt-get update
!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended
#!jupyter nbconvert --to html "/drive/MyDrive/movie_genre1.ipynb"

!jupyter nbconvert --log-level CRITICAL --to PDF "/content/movie_genre1.ipynb" # make sure the ipynb name is correct /content/last-names.txt
!jupyter nbconvert movie_genre1.ipynb --to pdf